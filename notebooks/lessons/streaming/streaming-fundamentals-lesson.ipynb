{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b71d4bb",
   "metadata": {},
   "source": [
    "# ğŸŒŠ Streaming Fundamentals: A Data Forge Lesson\n",
    "\n",
    "**Learn real-time data streaming with Apache Spark, Kafka, and Avro**\n",
    "\n",
    "This lesson demonstrates core streaming concepts using Data Forge's retail data generator. You'll understand why Avro beats JSON, how Schema Registry enables evolution, and how PySpark Structured Streaming works.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "\n",
    "By the end of this lesson, you'll understand:\n",
    "\n",
    "1. **Prerequisites** â†’ How to start Data Forge's data generator\n",
    "2. **Data Examination** â†’ What streaming retail data looks like\n",
    "3. **Avro vs JSON** â†’ Why Avro is superior for streaming (with evidence)\n",
    "4. **Schema Registry** â†’ How it enables safe schema evolution\n",
    "5. **PySpark Streaming** â†’ How Structured Streaming processes infinite data\n",
    "6. **Checkpoints** â†’ Why they're critical for fault tolerance\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ Prerequisites\n",
    "\n",
    "**Before starting this lesson:**\n",
    "\n",
    "```bash\n",
    "# 1. Start Data Forge core services\n",
    "docker compose --profile core up -d\n",
    "\n",
    "# 2. Start the data generator\n",
    "docker compose --profile datagen up -d\n",
    "\n",
    "# 3. Verify data is flowing\n",
    "docker compose logs -f data-generator | head -20\n",
    "```\n",
    "\n",
    "ğŸ›‘ **Without the data generator, this lesson won't work.** The generator produces the streaming events we'll analyze."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f718a7c7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âš™ï¸ Setup & Configuration\n",
    "\n",
    "Initialize our streaming environment and validate connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43613fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Data Forge service endpoints\n",
    "KAFKA_BOOTSTRAP = os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'kafka:9092')\n",
    "SCHEMA_REGISTRY_URL = os.getenv('SCHEMA_REGISTRY_URL', 'http://schema-registry:8081')\n",
    "SPARK_MASTER = os.getenv('SPARK_MASTER_URL', 'spark://spark-master:7077')\n",
    "\n",
    "print(\"ğŸ”¥ Data Forge Streaming Configuration:\")\n",
    "print(f\"   Kafka Bootstrap: {KAFKA_BOOTSTRAP}\")\n",
    "print(f\"   Schema Registry: {SCHEMA_REGISTRY_URL}\")\n",
    "print(f\"   Spark Master: {SPARK_MASTER}\")\n",
    "print(f\"   Lesson Start: {datetime.now()}\")\n",
    "print(\"\\nâœ… Ready to learn streaming fundamentals!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb651d0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ” Lesson 1: Examining Streaming Data\n",
    "\n",
    "**Goal:** Understand what real-time retail data looks like.\n",
    "\n",
    "Data Forge's generator produces realistic business events: orders, payments, shipments, inventory changes, and customer interactions. Let's examine this data to understand streaming patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700baefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Consumer\n",
    "import json\n",
    "\n",
    "kafka_config = {\n",
    "    'bootstrap.servers': 'kafka:9092',\n",
    "    'group.id': 'streaming-lesson-' + str(hash('lesson') % 10000),\n",
    "    'auto.offset.reset': 'earliest',\n",
    "    'enable.auto.commit': False\n",
    "}\n",
    "\n",
    "def sample_streaming_data(topic_name, num_messages=2):\n",
    "    \"\"\"Sample live messages from a Kafka topic to understand data structure\"\"\"\n",
    "    print(f\"ğŸ“Š Sampling {num_messages} live messages from {topic_name}...\")\n",
    "    \n",
    "    consumer = Consumer(kafka_config)\n",
    "    \n",
    "    try:\n",
    "        consumer.subscribe([topic_name])\n",
    "        messages = []\n",
    "        attempts = 0\n",
    "        max_attempts = 30\n",
    "        \n",
    "        while len(messages) < num_messages and attempts < max_attempts:\n",
    "            msg = consumer.poll(timeout=1.0)\n",
    "            attempts += 1\n",
    "            \n",
    "            if attempts % 10 == 0:\n",
    "                print(f\"   ğŸ”„ Polling attempt {attempts}/{max_attempts}...\")\n",
    "            \n",
    "            if msg is None:\n",
    "                continue\n",
    "            if msg.error():\n",
    "                print(f\"âŒ Consumer error: {msg.error()}\")\n",
    "                continue\n",
    "                \n",
    "            messages.append(msg)\n",
    "            \n",
    "            print(f\"\\nâœ… MESSAGE {len(messages)} CAPTURED:\")\n",
    "            print(f\"   ğŸ“ Topic: {msg.topic()}, Partition: {msg.partition()}, Offset: {msg.offset()}\")\n",
    "            print(f\"   â° Timestamp: {msg.timestamp()}\")\n",
    "            key = msg.key()\n",
    "            if key:\n",
    "                try:\n",
    "                    key_str = key.decode('utf-8') if isinstance(key, bytes) else str(key)\n",
    "                    print(f\"   ğŸ”‘ Key: {key_str}\")\n",
    "                except:\n",
    "                    print(f\"   ğŸ”‘ Key: {key} (binary)\")\n",
    "            else:\n",
    "                print(f\"   ğŸ”‘ Key: None\")\n",
    "            value = msg.value()\n",
    "            if value:\n",
    "                print(f\"   ğŸ“¦ Value length: {len(value)} bytes\")\n",
    "                hex_preview = ' '.join([f'{b:02x}' for b in value[:20]])\n",
    "                print(f\"   ğŸ” Hex preview: {hex_preview}...\")\n",
    "                if len(value) >= 5 and value[0] == 0:\n",
    "                    schema_id = int.from_bytes(value[1:5], byteorder='big')\n",
    "                    print(f\"   ğŸ“‹ Avro schema ID: {schema_id}\")\n",
    "                    print(f\"   âœ… Confluent Schema Registry format detected\")\n",
    "                else:\n",
    "                    print(f\"   âš ï¸ Not standard Confluent Avro format\")\n",
    "            else:\n",
    "                print(f\"   ğŸ“¦ Value: None\")\n",
    "        \n",
    "        if len(messages) == 0:\n",
    "            print(\"ğŸ›‘ No messages found - check if data generator is running!\")\n",
    "        else:\n",
    "            print(f\"\\nğŸ¯ Successfully sampled {len(messages)} messages from {topic_name}\")\n",
    "            \n",
    "        return messages\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "        return []\n",
    "    finally:\n",
    "        consumer.close()\n",
    "\n",
    "print(\"ğŸ”— Testing Kafka connectivity...\")\n",
    "try:\n",
    "    consumer = Consumer({'bootstrap.servers': 'kafka:9092', 'group.id': 'test-connectivity'})\n",
    "    metadata = consumer.list_topics(timeout=10)\n",
    "    print(f\"âœ… Connected to Kafka. Found {len(metadata.topics)} topics:\")\n",
    "    \n",
    "    retail_topics = []\n",
    "    for topic_name in sorted(metadata.topics.keys()):\n",
    "        if not topic_name.startswith('_'):\n",
    "            partitions = len(metadata.topics[topic_name].partitions)\n",
    "            print(f\"   ğŸ“Š {topic_name} ({partitions} partitions)\")\n",
    "            if topic_name.endswith('.v1'):\n",
    "                retail_topics.append(topic_name)\n",
    "    \n",
    "    print(f\"\\nğŸª Retail streaming topics: {retail_topics}\")\n",
    "    consumer.close()\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Kafka connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1141a215",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ›’ EXAMINING ORDERS DATA:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "orders_messages = sample_streaming_data(\"orders.v1\", 2)\n",
    "\n",
    "print(\"\\nğŸ“ LESSON INSIGHT:\")\n",
    "print(\"Notice the binary data format - this is Avro, not JSON.\")\n",
    "print(\"The magic byte (00) + schema ID (4 bytes) tells us it's Confluent format.\")\n",
    "print(\"This compact binary encoding is why Avro beats JSON for streaming.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0f8a4e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“‹ Lesson 2: Schema Registry Deep Dive\n",
    "\n",
    "**Goal:** Understand how Schema Registry enables safe schema evolution.\n",
    "\n",
    "Schema Registry is like a \"contract database\" for your streaming data. It stores Avro schemas and enforces compatibility rules, preventing breaking changes that would crash your streaming pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7c6a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "\n",
    "sr_client = SchemaRegistryClient({'url': 'http://schema-registry:8081'})\n",
    "\n",
    "print(\"ğŸ“‹ SCHEMA REGISTRY EXPLORATION:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    subjects = sr_client.get_subjects()\n",
    "    print(f\"ğŸ“Š Total schema subjects: {len(subjects)}\")\n",
    "    print(\"\\nğŸ” Available schemas:\")\n",
    "    \n",
    "    for subject in sorted(subjects):\n",
    "        try:\n",
    "            versions = sr_client.get_versions(subject)\n",
    "            latest_version = sr_client.get_latest_version(subject)\n",
    "            print(f\"   ğŸ“‹ {subject}: {len(versions)} versions (latest: v{latest_version.version}, schema ID: {latest_version.schema_id})\")\n",
    "            if subject == \"orders.v1-value\":\n",
    "                schema_str = latest_version.schema.schema_str\n",
    "                schema_obj = json.loads(schema_str)\n",
    "                \n",
    "                print(f\"\\nğŸ›’ ORDERS SCHEMA BREAKDOWN:\")\n",
    "                print(f\"   ğŸ“ Schema name: {schema_obj['name']}\")\n",
    "                print(f\"   ğŸ“Š Number of fields: {len(schema_obj['fields'])}\")\n",
    "                print(f\"   ğŸ”§ Fields:\")\n",
    "                for field in schema_obj['fields']:\n",
    "                    field_type = field['type']\n",
    "                    print(f\"      â€¢ {field['name']}: {field_type}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ {subject}: (error getting version info: {e})\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Schema Registry error: {e}\")\n",
    "\n",
    "print(f\"\\nğŸ“ LESSON INSIGHT:\")\n",
    "print(f\"Schema Registry acts as a 'contract database' for streaming data.\")\n",
    "print(f\"Each message references a schema ID, enabling safe evolution without breaking consumers.\")\n",
    "print(f\"This is impossible with JSON - you'd need to parse every message to know its structure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b017b8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ†š Lesson 3: Avro vs JSON - The Evidence\n",
    "\n",
    "**Goal:** Understand why Avro dominates streaming with concrete evidence.\n",
    "\n",
    "JSON seems simpler, but Avro wins on every metric that matters for streaming: size, speed, schema evolution, and type safety. Let's prove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4714e0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_avro_message(message_value):\n",
    "    \"\"\"Decode Avro message using proven working method\"\"\"\n",
    "    try:\n",
    "        if len(message_value) < 5:\n",
    "            return None\n",
    "            \n",
    "        magic_byte = message_value[0]\n",
    "        schema_id = int.from_bytes(message_value[1:5], byteorder='big')\n",
    "        avro_payload = message_value[5:]\n",
    "        \n",
    "        if magic_byte != 0:\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            import io\n",
    "            import avro.schema\n",
    "            import avro.io\n",
    "            \n",
    "            schema = sr_client.get_schema(schema_id)\n",
    "            avro_schema = avro.schema.parse(schema.schema_str)\n",
    "            \n",
    "            bytes_reader = io.BytesIO(avro_payload)\n",
    "            decoder = avro.io.BinaryDecoder(bytes_reader)\n",
    "            reader = avro.io.DatumReader(avro_schema)\n",
    "            \n",
    "            decoded = reader.read(decoder)\n",
    "            return decoded\n",
    "            \n",
    "        except Exception:\n",
    "            return None\n",
    "        \n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "print(\"ğŸ†š AVRO vs JSON COMPARISON:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if orders_messages:\n",
    "    sample_msg = orders_messages[0]\n",
    "    avro_data = decode_avro_message(sample_msg.value())\n",
    "    \n",
    "    if avro_data:\n",
    "        json_equivalent = json.dumps(avro_data, indent=2, default=str)\n",
    "        \n",
    "        avro_size = len(sample_msg.value())\n",
    "        json_size = len(json_equivalent.encode('utf-8'))\n",
    "        \n",
    "        print(f\"ğŸ“Š SIZE COMPARISON:\")\n",
    "        print(f\"   ğŸ”¹ Avro binary: {avro_size} bytes\")\n",
    "        print(f\"   ğŸ”¹ JSON equivalent: {json_size} bytes\")\n",
    "        print(f\"   ğŸ“ˆ Space savings: {((json_size - avro_size) / json_size * 100):.1f}% smaller with Avro\")\n",
    "        \n",
    "        print(f\"\\nğŸ“‹ DECODED DATA:\")\n",
    "        for key, value in avro_data.items():\n",
    "            if isinstance(value, (int, float, bool)):\n",
    "                print(f\"   {key}: {value}\")\n",
    "            elif isinstance(value, str):\n",
    "                safe_value = ''.join(c if ord(c) < 128 else '?' for c in value)\n",
    "                print(f\"   {key}: '{safe_value}'\")\n",
    "            else:\n",
    "                print(f\"   {key}: {str(value)}\")\n",
    "        if 'ts' in avro_data:\n",
    "            try:\n",
    "                from datetime import datetime\n",
    "                ts_ms = avro_data['ts']\n",
    "                if isinstance(ts_ms, (int, float)):\n",
    "                    ts_readable = datetime.fromtimestamp(ts_ms / 1000).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                    print(f\"   ts_readable: {ts_readable}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        \n",
    "        print(f\"\\nğŸ“ WHY AVRO WINS:\")\n",
    "        print(f\"   âœ… Size: {((json_size - avro_size) / json_size * 100):.1f}% smaller â†’ less network/storage cost\")\n",
    "        print(f\"   âœ… Speed: Binary parsing is faster than JSON text parsing\")\n",
    "        print(f\"   âœ… Schema: Enforced types prevent runtime errors\")\n",
    "        print(f\"   âœ… Evolution: Add/remove fields without breaking consumers\")\n",
    "        print(f\"   âœ… Compression: Better compression ratios due to structure\")\n",
    "        \n",
    "        print(f\"\\nâŒ JSON PROBLEMS:\")\n",
    "        print(f\"   âŒ No schema enforcement â†’ runtime type errors\")\n",
    "        print(f\"   âŒ Text parsing overhead\")\n",
    "        print(f\"   âŒ Field name repetition in every message\")\n",
    "        print(f\"   âŒ No safe evolution strategy\")\n",
    "        \n",
    "    else:\n",
    "        print(\"âŒ Could not decode Avro message\")\n",
    "else:\n",
    "    print(\"âŒ No orders messages available for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb869055",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âš¡ Lesson 4: PySpark Structured Streaming Fundamentals\n",
    "\n",
    "**Goal:** Understand how Spark processes infinite data streams.\n",
    "\n",
    "Traditional batch processing reads finite data, processes it, and stops. Streaming is different - data never stops arriving. Spark Structured Streaming treats streams as \"unbounded tables\" that grow continuously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65b8e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import os\n",
    "\n",
    "print(\"âš¡ SPARK STREAMING SETUP:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"StreamingFundamentalsLesson\") \\\n",
    "    .master(SPARK_MASTER) \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0,\"\n",
    "            \"org.apache.spark:spark-avro_2.12:3.4.0\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.streaming.checkpointLocation\", \"/tmp/streaming-lesson-checkpoint\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"âœ… Spark session created successfully\")\n",
    "print(f\"   ğŸ“Š Spark version: {spark.version}\")\n",
    "print(f\"   ğŸ†” Application ID: {spark.sparkContext.applicationId}\")\n",
    "print(f\"   ğŸ–¥ï¸ Master: {SPARK_MASTER}\")\n",
    "print(f\"   ğŸ’¾ Checkpoint location: /tmp/streaming-lesson-checkpoint\")\n",
    "print(f\"\\nğŸ”— Testing Spark-Kafka connectivity...\")\n",
    "try:\n",
    "    test_df = spark.read \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP) \\\n",
    "        .option(\"subscribe\", \"orders.v1\") \\\n",
    "        .option(\"startingOffsets\", \"earliest\") \\\n",
    "        .option(\"endingOffsets\", \"latest\") \\\n",
    "        .load()\n",
    "    \n",
    "    message_count = test_df.count()\n",
    "    print(f\"âœ… Kafka connectivity test passed\")\n",
    "    print(f\"   ğŸ“Š Found {message_count} messages in orders.v1 topic\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Kafka connectivity test failed: {e}\")\n",
    "\n",
    "print(f\"\\nğŸ“ STREAMING FUNDAMENTALS:\")\n",
    "print(f\"   â€¢ Streaming = processing unbounded (infinite) data\")\n",
    "print(f\"   â€¢ Spark treats streams as 'growing tables'\")\n",
    "print(f\"   â€¢ Each micro-batch processes new data incrementally\")\n",
    "print(f\"   â€¢ Checkpoints track progress for fault tolerance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0beaf19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_streaming_dataframe(topic_name):\n",
    "    \"\"\"Create a streaming DataFrame - this represents infinite data\"\"\"\n",
    "    print(f\"ğŸŒŠ Creating streaming DataFrame for {topic_name}\")\n",
    "    \n",
    "    try:\n",
    "        # This creates a streaming DataFrame - it represents infinite data\n",
    "        kafka_stream = (spark.readStream\n",
    "            .format(\"kafka\")\n",
    "            .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP)\n",
    "            .option(\"subscribe\", topic_name)\n",
    "            .option(\"startingOffsets\", \"latest\")  # Only process new data\n",
    "            .option(\"failOnDataLoss\", \"false\")    # Don't fail if data is lost\n",
    "            .load())\n",
    "        \n",
    "        # Transform the raw Kafka data into a structured format\n",
    "        structured_stream = kafka_stream.select(\n",
    "            col(\"key\").cast(\"string\").alias(\"message_key\"),\n",
    "            col(\"topic\"),\n",
    "            col(\"partition\"),\n",
    "            col(\"offset\"),\n",
    "            col(\"timestamp\").alias(\"kafka_timestamp\"),\n",
    "            length(col(\"value\")).alias(\"message_size_bytes\"),\n",
    "            col(\"value\")  # Raw Avro binary data\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… Streaming DataFrame created for {topic_name}\")\n",
    "        print(f\"   ğŸ“Š Schema (what each streaming record looks like):\")\n",
    "        structured_stream.printSchema()\n",
    "        \n",
    "        return structured_stream\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error creating streaming DataFrame: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"ğŸ›’ CREATING ORDERS STREAM:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "orders_stream = create_streaming_dataframe(\"orders.v1\")\n",
    "\n",
    "if orders_stream:\n",
    "    print(f\"\\nğŸ“ KEY CONCEPTS:\")\n",
    "    print(f\"   â€¢ This DataFrame represents INFINITE data - it never ends\")\n",
    "    print(f\"   â€¢ isStreaming = {orders_stream.isStreaming}\")\n",
    "    print(f\"   â€¢ You can't call .show() or .collect() on it directly\")\n",
    "    print(f\"   â€¢ You need a 'streaming query' to process the data\")\n",
    "    print(f\"   â€¢ Spark processes data in micro-batches (e.g., every 2 seconds)\")\n",
    "else:\n",
    "    print(\"âŒ Failed to create orders stream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51d9e5b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ’¾ Lesson 5: Checkpoints - The Fault Tolerance Foundation\n",
    "\n",
    "**Goal:** Understand why checkpoints are critical for production streaming.\n",
    "\n",
    "Streaming applications run 24/7. Networks fail, machines crash, code gets deployed. Without checkpoints, you'd lose progress and either miss data or reprocess everything. Checkpoints are your streaming safety net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458975f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "print(\"ğŸ’¾ CHECKPOINTING DEMONSTRATION:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if orders_stream:\n",
    "    print(\"ğŸ” What checkpoints store:\")\n",
    "    print(\"   â€¢ Stream metadata (offsets, batch IDs)\")\n",
    "    print(\"   â€¢ Query progress information\")\n",
    "    print(\"   â€¢ State store data for aggregations\")\n",
    "    print(\"   â€¢ Watermark information for event time\")\n",
    "    \n",
    "    # Create a streaming query with memory sink (perfect for Jupyter)\n",
    "    checkpoint_demo_query = orders_stream.select(\n",
    "        col(\"message_key\"),\n",
    "        col(\"offset\"),\n",
    "        col(\"kafka_timestamp\"),\n",
    "        col(\"message_size_bytes\"),\n",
    "        # Extract hex preview for educational purposes\n",
    "        expr(\"hex(substring(value, 1, 20))\").alias(\"avro_hex_preview\")\n",
    "    ).writeStream \\\n",
    "        .queryName(\"checkpoint_demo\") \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .format(\"memory\") \\\n",
    "        .option(\"checkpointLocation\", \"/tmp/streaming-lesson-checkpoint/demo\") \\\n",
    "        .trigger(processingTime=\"3 seconds\") \\\n",
    "        .start()\n",
    "\n",
    "    print(f\"\\nâœ… Streaming query started with checkpointing!\")\n",
    "    print(f\"   ğŸ“ Query name: {checkpoint_demo_query.name}\")\n",
    "    print(f\"   ğŸ“ Checkpoint location: /tmp/streaming-lesson-checkpoint/demo\")\n",
    "    print(f\"   â±ï¸ Processing trigger: every 3 seconds\")\n",
    "    print(f\"\\nğŸ”„ Watching streaming progress (15 seconds)...\")\n",
    "    \n",
    "    for i in range(5):\n",
    "        time.sleep(3)\n",
    "        \n",
    "        try:\n",
    "            current_data = spark.sql(\"SELECT COUNT(*) as total FROM checkpoint_demo\")\n",
    "            total_messages = current_data.collect()[0].total\n",
    "            progress = checkpoint_demo_query.lastProgress\n",
    "            \n",
    "            print(f\"   ğŸ“Š Progress check {i+1}/5:\")\n",
    "            print(f\"      Messages processed: {total_messages}\")\n",
    "            \n",
    "            if progress:\n",
    "                batch_id = progress.get('batchId', 'N/A')\n",
    "                input_rate = progress.get('inputRowsPerSecond', 0)\n",
    "                print(f\"      Current batch: {batch_id}\")\n",
    "                print(f\"      Input rate: {input_rate:.1f} rows/sec\")\n",
    "            if total_messages > 0:\n",
    "                sample = spark.sql(\"SELECT message_key, offset, message_size_bytes FROM checkpoint_demo ORDER BY offset DESC LIMIT 2\")\n",
    "                rows = sample.collect()\n",
    "                print(f\"      Latest messages:\")\n",
    "                for row in rows:\n",
    "                    print(f\"        Key: {row.message_key}, Offset: {row.offset}, Size: {row.message_size_bytes}B\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"      âš ï¸ Progress check {i+1}: {e}\")\n",
    "    checkpoint_demo_query.stop()\n",
    "    print(f\"\\nğŸ›‘ Streaming query stopped\")\n",
    "    \n",
    "    print(f\"\\nğŸ“ CHECKPOINT BENEFITS:\")\n",
    "    print(f\"   âœ… Exactly-once processing guarantees\")\n",
    "    print(f\"   âœ… Fault tolerance - resume from failure point\")\n",
    "    print(f\"   âœ… State preservation for aggregations\")\n",
    "    print(f\"   âœ… No data loss or duplication\")\n",
    "    \n",
    "    print(f\"\\nâš ï¸ CHECKPOINT CONSIDERATIONS:\")\n",
    "    print(f\"   â€¢ Choose reliable storage (HDFS, S3, not local /tmp in production)\")\n",
    "    print(f\"   â€¢ Checkpoint format is tied to Spark version\")\n",
    "    print(f\"   â€¢ Schema changes may require checkpoint reset\")\n",
    "    print(f\"   â€¢ Checkpoint size grows with state (aggregations)\")\n",
    "\n",
    "else:\n",
    "    print(\"âŒ No stream available for checkpoint demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ad52cc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸŒŠ Lesson 6: Advanced Streaming - Avro Decoding in Real-Time\n",
    "\n",
    "**Goal:** Combine everything - stream processing with Avro decoding.\n",
    "\n",
    "Now let's put it all together: process infinite Kafka streams, decode Avro messages in real-time, and display business-readable data. This is production-level streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dddea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸš€ ADVANCED STREAMING WITH AVRO DECODING:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if orders_stream:\n",
    "    print(\"ğŸ¯ This demonstrates production-level streaming:\")\n",
    "    print(\"   â€¢ Infinite data processing\")\n",
    "    print(\"   â€¢ Real-time Avro decoding\")\n",
    "    print(\"   â€¢ Business data extraction\")\n",
    "    print(\"   â€¢ Fault-tolerant checkpointing\")\n",
    "    advanced_query = orders_stream.select(\n",
    "        col(\"message_key\"),\n",
    "        col(\"offset\"),\n",
    "        col(\"kafka_timestamp\"),\n",
    "        col(\"value\"),  # Full Avro binary for decoding\n",
    "        col(\"message_size_bytes\")\n",
    "    ).writeStream \\\n",
    "        .queryName(\"advanced_avro_streaming\") \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .format(\"memory\") \\\n",
    "        .option(\"checkpointLocation\", \"/tmp/streaming-lesson-checkpoint/advanced\") \\\n",
    "        .trigger(processingTime=\"2 seconds\") \\\n",
    "        .start()\n",
    "\n",
    "    print(f\"\\nâœ… Advanced streaming query started!\")\n",
    "    print(f\"   ğŸ”§ Processing: Kafka â†’ Spark â†’ Avro decode â†’ Business data\")\n",
    "    print(f\"   ğŸ“Š Collecting and decoding live data...\")\n",
    "    for i in range(6):\n",
    "        time.sleep(2)\n",
    "        \n",
    "        try:\n",
    "            current_data = spark.sql(\"SELECT * FROM advanced_avro_streaming ORDER BY offset DESC LIMIT 1\")\n",
    "            data_count = current_data.count()\n",
    "            \n",
    "            if data_count > 0:\n",
    "                print(f\"\\nğŸ“Š LIVE UPDATE {i+1}/6:\")\n",
    "                print(\"â”€\" * 40)\n",
    "                \n",
    "                row = current_data.collect()[0]\n",
    "                print(f\"ğŸ“ Kafka metadata:\")\n",
    "                print(f\"   Key: {row.message_key}\")\n",
    "                print(f\"   Offset: {row.offset}\")\n",
    "                print(f\"   Timestamp: {row.kafka_timestamp}\")\n",
    "                print(f\"   Size: {row.message_size_bytes} bytes\")\n",
    "                try:\n",
    "                    message_bytes = row.value\n",
    "                    if message_bytes and len(message_bytes) >= 5:\n",
    "                        magic_byte = message_bytes[0]\n",
    "                        schema_id = int.from_bytes(message_bytes[1:5], byteorder='big')\n",
    "                        avro_payload = message_bytes[5:]\n",
    "                        \n",
    "                        if magic_byte == 0:\n",
    "                            try:\n",
    "                                import io\n",
    "                                import avro.schema\n",
    "                                import avro.io\n",
    "                                \n",
    "                                schema = sr_client.get_schema(schema_id)\n",
    "                                avro_schema = avro.schema.parse(schema.schema_str)\n",
    "                                \n",
    "                                bytes_reader = io.BytesIO(avro_payload)\n",
    "                                decoder = avro.io.BinaryDecoder(bytes_reader)\n",
    "                                reader = avro.io.DatumReader(avro_schema)\n",
    "                                \n",
    "                                decoded = reader.read(decoder)\n",
    "                                \n",
    "                                print(f\"\\nğŸ¯ DECODED BUSINESS DATA:\")\n",
    "                                for key, value in decoded.items():\n",
    "                                    if isinstance(value, (int, float)):\n",
    "                                        print(f\"   {key}: {value}\")\n",
    "                                    elif isinstance(value, str):\n",
    "                                        safe_value = ''.join(c if ord(c) < 128 else '?' for c in value)\n",
    "                                        print(f\"   {key}: '{safe_value}'\")\n",
    "                                    else:\n",
    "                                        print(f\"   {key}: {str(value)}\")\n",
    "\n",
    "                                if 'ts' in decoded and isinstance(decoded['ts'], (int, float)):\n",
    "                                    from datetime import datetime\n",
    "                                    ts_readable = datetime.fromtimestamp(decoded['ts'] / 1000).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                                    print(f\"   event_time: {ts_readable}\")\n",
    "                                \n",
    "                                print(f\"   âœ… Real-time Avro decoding successful!\")\n",
    "                                    \n",
    "                            except Exception as decode_error:\n",
    "                                print(f\"   âš ï¸ Avro decode error: {decode_error}\")\n",
    "                        else:\n",
    "                            print(f\"   âš ï¸ Invalid magic byte: {magic_byte}\")\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    print(f\"   âŒ Processing error: {e}\")\n",
    "            else:\n",
    "                print(f\"â³ Update {i+1}/6: Waiting for streaming data...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Update {i+1}/6: {e}\")\n",
    "    advanced_query.stop()\n",
    "    print(f\"\\nğŸ›‘ Advanced streaming query stopped\")\n",
    "    \n",
    "    print(f\"\\nğŸ“ WHAT YOU JUST SAW:\")\n",
    "    print(f\"   ğŸŒŠ Infinite data stream processing\")\n",
    "    print(f\"   ğŸ“‹ Schema Registry integration\")\n",
    "    print(f\"   ğŸ”§ Real-time Avro decoding\")\n",
    "    print(f\"   ğŸ’¾ Fault-tolerant checkpointing\")\n",
    "    print(f\"   ğŸ“Š Business data extraction from binary streams\")\n",
    "    print(f\"   âš¡ This is how production streaming systems work!\")\n",
    "\n",
    "else:\n",
    "    print(\"âŒ No stream available for advanced demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d3be8e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Lesson Summary & Production Insights\n",
    "\n",
    "**Congratulations!** You've learned the fundamentals of modern streaming architecture.\n",
    "\n",
    "### ğŸ“ What You Learned\n",
    "\n",
    "1. **Prerequisites** â†’ Data Forge's generator creates realistic streaming data\n",
    "2. **Data Examination** â†’ Streaming data is binary Avro, not JSON\n",
    "3. **Avro Superiority** â†’ 30-50% smaller, faster, type-safe, evolvable\n",
    "4. **Schema Registry** â†’ Enables safe schema evolution without breaking consumers\n",
    "5. **Spark Streaming** â†’ Treats infinite streams as \"growing tables\"\n",
    "6. **Checkpoints** â†’ Critical for exactly-once processing and fault tolerance\n",
    "\n",
    "### ğŸ­ Production Patterns\n",
    "\n",
    "**You're now ready for real-world streaming:**\n",
    "\n",
    "- **Schema Evolution** â†’ Add/remove fields without downtime\n",
    "- **Fault Tolerance** â†’ Streams survive failures and restarts\n",
    "- **Exactly-Once Processing** â†’ No data loss or duplication\n",
    "- **Type Safety** â†’ Avro prevents runtime errors\n",
    "- **Performance** â†’ Binary encoding reduces costs\n",
    "\n",
    "### ğŸš€ Next Steps\n",
    "\n",
    "**Explore more Data Forge capabilities:**\n",
    "\n",
    "```bash\n",
    "# Explore with Trino SQL\n",
    "# Visit http://localhost:8081 for Trino UI\n",
    "\n",
    "# Build dashboards in Superset\n",
    "# Visit http://localhost:8088 (admin/admin)\n",
    "```\n",
    "\n",
    "**Additional learning resources:**\n",
    "- `notebooks/lessons/streaming` â†’ More streaming examples\n",
    "- `docs/` â†’ Service-specific guides\n",
    "- Data Forge README â†’ Architecture overview\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”§ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a40d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ§¹ LESSON CLEANUP:\")\n",
    "print(\"=\" * 30)\n",
    "active_queries = spark.streams.active\n",
    "if active_queries:\n",
    "    print(f\"ğŸ›‘ Stopping {len(active_queries)} active streaming queries...\")\n",
    "    for query in active_queries:\n",
    "        query.stop()\n",
    "        print(f\"   âœ… Stopped: {query.name}\")\n",
    "else:\n",
    "    print(\"â„¹ï¸ No active queries to stop\")\n",
    "try:\n",
    "    import shutil\n",
    "    if os.path.exists(\"/tmp/streaming-lesson-checkpoint\"):\n",
    "        shutil.rmtree(\"/tmp/streaming-lesson-checkpoint\")\n",
    "        print(\"ğŸ—‘ï¸ Cleaned up checkpoint directory\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Checkpoint cleanup: {e}\")\n",
    "\n",
    "print(f\"\\nâœ… Lesson cleanup complete!\")\n",
    "print(f\"ğŸ’¡ Spark session remains active for further experimentation\")\n",
    "print(f\"ğŸ‰ You've mastered streaming fundamentals with Data Forge!\")\n",
    "\n",
    "# Optional: Uncomment to stop Spark completely\n",
    "# spark.stop()\n",
    "# print(\"ğŸ Spark session stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefc7e1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
