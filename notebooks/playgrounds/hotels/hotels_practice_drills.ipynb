{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🧊 Hotels Practice Drills\n",
        "\n",
        "Use this lab to run the SQL, PySpark, and Python exercises against the hotels practice dataset. The first two cells spin up Spark with Iceberg support and configure Trino access. All subsequent sections list tasks only—add your own cells beneath each bullet to implement solutions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "MINIO_ENDPOINT = os.getenv(\"MINIO_ENDPOINT\", \"http://minio:9000\")\n",
        "MINIO_ACCESS_KEY = os.getenv(\"MINIO_ROOT_USER\", \"minio\")\n",
        "MINIO_SECRET_KEY = os.getenv(\"MINIO_ROOT_PASSWORD\", \"minio123\")\n",
        "HIVE_METASTORE_URI = os.getenv(\"HIVE_METASTORE_URI\", \"thrift://hive-metastore:9083\")\n",
        "TRINO_URL = os.getenv(\"TRINO_URL\", \"http://trino:8080\")\n",
        "SPARK_MASTER = os.getenv(\"SPARK_MASTER_URL\", \"spark://spark-master:7077\")\n",
        "S3_ENDPOINT = os.getenv(\"S3_ENDPOINT\", \"minio:9000\")\n",
        "\n",
        "os.environ.setdefault(\"AWS_REGION\", \"us-east-1\")\n",
        "os.environ.setdefault(\"AWS_DEFAULT_REGION\", os.environ[\"AWS_REGION\"])\n",
        "\n",
        "print(\"Spark master:\", SPARK_MASTER)\n",
        "print(\"Trino URL:\", TRINO_URL)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "packages = [\n",
        "    \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.9.2\",\n",
        "    \"org.apache.hadoop:hadoop-aws:3.3.4\",\n",
        "    \"software.amazon.awssdk:bundle:2.20.158\",\n",
        "]\n",
        "\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "    .appName(\"HotelsPracticeDrills\")\n",
        "    .master(SPARK_MASTER)\n",
        "    .config(\"spark.jars.packages\", \",\".join(packages))\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\")\n",
        "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hive\")\n",
        "    .config(\"spark.sql.catalog.iceberg\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
        "    .config(\"spark.sql.catalog.iceberg.type\", \"rest\")\n",
        "    .config(\"spark.sql.catalog.iceberg.uri\", \"http://hive-metastore:9001/iceberg\")\n",
        "    .config(\"spark.sql.catalog.iceberg.warehouse\", \"s3a://iceberg/warehouse\")\n",
        "    .config(\"spark.sql.catalog.iceberg.s3.endpoint\", f\"http://{S3_ENDPOINT}\")\n",
        "    .config(\"spark.sql.catalog.iceberg.s3.access-key-id\", MINIO_ACCESS_KEY)\n",
        "    .config(\"spark.sql.catalog.iceberg.s3.secret-access-key\", MINIO_SECRET_KEY)\n",
        "    .config(\"spark.sql.catalog.iceberg.s3.region\", os.environ[\"AWS_REGION\"])\n",
        "    .config(\"spark.sql.catalog.iceberg.s3.path-style-access\", \"true\")\n",
        "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
        "    .config(\"spark.sql.defaultCatalog\", \"iceberg\")\n",
        "    .enableHiveSupport()\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
        "\n",
        "print(\"Spark session ready:\", spark.version)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import trino\n",
        "\n",
        "trino_conn = trino.dbapi.connect(\n",
        "    host=os.getenv(\"TRINO_HOST\", \"trino\"),\n",
        "    port=int(os.getenv(\"TRINO_PORT\", \"8080\")),\n",
        "    user=os.getenv(\"TRINO_USER\", \"admin\"),\n",
        "    catalog=os.getenv(\"TRINO_CATALOG\", \"iceberg\"),\n",
        "    schema=os.getenv(\"TRINO_SCHEMA\", \"hotels_practice\"),\n",
        ")\n",
        "print(\"Connected to Trino catalog/schema:\", trino_conn.schema)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🧮 Section A — SQL (Trino) Drills\n",
        "\n",
        "Catalog/schema default: `iceberg.hotels_practice`. Adjust queries if you wrote tables elsewhere.\n",
        "\n",
        "### A1. Top Hotels by Recent Review Quality (last 180 days)\n",
        "- Inputs: `reviews`, `hotels`\n",
        "- Compute `review_cnt`, `avg_rating` grouped by `(country, city, hotel_id)` for reviews in the last 180 days.\n",
        "- Return the top 20 ordered by `review_cnt` desc, then `avg_rating` desc. Handle ties safely.\n",
        "\n",
        "### A2. Monthly Occupancy Proxy\n",
        "- Inputs: `bookings`, `hotels`\n",
        "- For `status = 'completed'`, calculate monthly `bookings` and `total_nights` per `(country, city, hotel_id, month)` using `date_trunc('month', checkin_date)`.\n",
        "- Order by `month` desc, then `bookings` desc.\n",
        "\n",
        "### A3. Top-3 Hotels per Country (Window)\n",
        "- Inputs: `bookings`, `hotels`\n",
        "- For completed bookings, compute total bookings and avg price per `(country, hotel_id)`.\n",
        "- Return at most three rows per country using window functions (e.g., `row_number()` or `dense_rank()` when ties should retain peers).\n",
        "\n",
        "### A4. Rolling Rating per Hotel (Window Frame)\n",
        "- Inputs: `reviews`\n",
        "- For each `hotel_id`, compute a rolling average of `rating` over the previous 10 rows ordered by `created_at`.\n",
        "\n",
        "### A5. Cancellation & No-Show Rates\n",
        "- Inputs: `bookings`, `hotels`\n",
        "- For each `(country, month)` compute cancel and no-show rates using `NULLIF(total, 0)` to avoid divide-by-zero.\n",
        "\n",
        "### A6. Review Language Mix & Bias Check\n",
        "- Inputs: `reviews`, `hotels`\n",
        "- Last 90 days, compute language share per country and return those with any language share > 0.6.\n",
        "\n",
        "### A7. Joining Images for Quality Screening\n",
        "- Inputs: `images`, `hotels`\n",
        "- Last 60 days, compute `avg_quality`, `image_cnt` per `(country, hotel_id)` and filter to `image_cnt >= 5` and `avg_quality >= 0.8`.\n",
        "\n",
        "### A8. “Trusted” Hotel Surface (Multi-signal)\n",
        "- Inputs: `hotels`, `reviews`, `bookings`, `images`\n",
        "- Build a view/query where hotels satisfy all of:\n",
        "  - ≥ 30 reviews in last 180 days with `avg_rating >= 4.2`\n",
        "  - ≥ 20 completed bookings in last 180 days\n",
        "  - ≥ 5 images in last 90 days with `avg_quality >= 0.75`\n",
        "- Compose with CTEs then join.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔥 Section B — PySpark Drills\n",
        "\n",
        "Load tables via `spark.table(\"iceberg.hotels_practice.<table>\")` or `spark.sql`.\n",
        "\n",
        "### B1. Sessionize Reviews by User (30-minute gaps)\n",
        "- Build sessions per `user_id` using window + `lag` to reset when gap > 30 minutes.\n",
        "\n",
        "### B2. Late-arriving Event Simulation\n",
        "- Use `spark.readStream` with a rate source or file source and the static reviews data to demonstrate a 5-minute tumbling aggregation with a 10-minute watermark.\n",
        "\n",
        "### B3. Skew Handling in Joins (Hotels × Reviews)\n",
        "- Join reviews to hotels and compute avg rating per hotel. Show one skew mitigation strategy (broadcast or salting).\n",
        "\n",
        "### B4. Partition-Aware Writes (Iceberg)\n",
        "- Rewrite reviews into a temp Iceberg table partitioned by `months(created_at)` and set a target file size property. Inspect metadata/EXPLAIN.\n",
        "\n",
        "### B5. Deduplicate Near-Duplicates by Text Fingerprint\n",
        "- Normalize `review_text` (lowercase, strip punctuation) and drop duplicates by `(hotel_id, fingerprint)`.\n",
        "\n",
        "### B6. Curate Balanced ML Training Slices\n",
        "- Sample balanced subsets across rating buckets (1–5) and languages (`en,de,fr,es,it,he`) with max N per bucket, writing to `iceberg.hotels_practice.ml_reviews_balanced`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🧠 Section C — Python / Data Prep Drills\n",
        "\n",
        "You can use pandas or PySpark DataFrames. Parse `review_metadata` JSON as needed.\n",
        "\n",
        "### C1. JSON Tag Explosion + Top Tags per Hotel\n",
        "- Parse tags from `review_metadata`, handle invalid JSON, and compute top three tags per hotel.\n",
        "\n",
        "### C2. Text Cleaning + Chunking for SFT\n",
        "- Normalize `review_text`, strip emojis/punctuation, and chunk into ~60-word segments per review.\n",
        "\n",
        "### C3. Language Filter + Coverage Report\n",
        "- Keep languages in `{en,de,fr,es,it,he}`. Report coverage % per lang and top 10 hotels by distinct language count.\n",
        "\n",
        "### C4. Toxicity/PII Placeholder Filter\n",
        "- Implement simple regex filters for profanity, emails, phone numbers. Output counts of filtered rows.\n",
        "\n",
        "### C5. Train/Val/Test Split by Hotel + Time\n",
        "- For each hotel, assign latest month to test, previous month to val, rest to train. Ensure no leakage.\n",
        "\n",
        "### C6. Simple Embedding Cache Index (Mock)\n",
        "- Build TF-IDF vectors for chunks (from C2), store metadata mapping, and implement a cosine-similarity top-K search helper.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ☁️ Section D — Advanced Dataset Workloads (Iceberg Only)\n",
        "\n",
        "All tasks rely exclusively on the tables produced by `hotels_iceberg_population.ipynb` (namespace default: `iceberg.hotels_practice`).\n",
        "\n",
        "### D1. Historical Snapshot Audits\n",
        "- Use Iceberg time travel to compare yesterday's snapshot with today and report row count deltas per table.\n",
        "\n",
        "### D2. Partition Health Checks\n",
        "- For `bookings` and `reviews`, compute partition sizes (`months(checkin_date)` / `months(created_at)`) and flag skewed partitions (e.g., >3× median).\n",
        "\n",
        "### D3. Compaction Strategy Proposal\n",
        "- Analyze file counts/average file sizes via `table.files` metadata and outline a compaction cadence (commands + triggers).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔄 Section E — Streaming Simulation on Existing Data\n",
        "\n",
        "### E1. Micro-batch Replay\n",
        "- Use Structured Streaming with the static `reviews` table as a rate-limited source (e.g., `spark.readStream.format(\"iceberg\").load(...)`).\n",
        "- Demonstrate watermarking and exactly-once upserts back into an Iceberg staging table.\n",
        "\n",
        "### E2. Late Data Handling\n",
        "- Inject artificial delays by duplicating rows with older timestamps; verify logic handles duplicates without off-stack sources.\n",
        "\n",
        "### E3. Quality Metrics\n",
        "- Compute per-batch metrics (records processed, duplicates dropped) and write them to `iceberg.hotels_practice.stream_metrics`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ✅ Section F — Data Quality & Governance\n",
        "\n",
        "### F1. Automated Profiling\n",
        "- Write profiling helpers that operate on the Iceberg tables (min/max, distinct counts, null ratios) and persist to `data_quality_profile`.\n",
        "\n",
        "### F2. Contract Enforcement\n",
        "- Express a contract for `bookings` as code (dictionary). Validate nightly and log violations to `data_quality_violations`.\n",
        "\n",
        "### F3. Catalog Documentation\n",
        "- Build a small metadata table with columns (table_name, description, owner, quality_score, sample_query) sourced from the current dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🧪 Section G — Experimentation & Monitoring (Dataset Only)\n",
        "\n",
        "### G1. Prompt/Response Logging Stub\n",
        "- Create a framework that logs generated summaries derived from `reviews` into `genai_prompt_log` (Iceberg table).\n",
        "- Include latency, prompt hash, response hash columns.\n",
        "\n",
        "### G2. Offline Metrics\n",
        "- Compute text similarity metrics between review_text and generated summaries using only local libraries (e.g., cosine over TF-IDF).\n",
        "- Store metrics per run in `genai_eval_metrics`.\n",
        "\n",
        "### G3. A/B Simulation\n",
        "- Split hotels into pseudo A/B cohorts using existing data; compute uplift in review conversion or image engagement using window functions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ☕ Section H — JVM & API Exercises within Dataset Scope\n",
        "\n",
        "### H1. Scala/Java Spark Translation\n",
        "- Translate the PySpark booking aggregation into Scala/Java using the same Iceberg tables (include code snippet or sbt skeleton).\n",
        "\n",
        "### H2. Data Access Service Sketch\n",
        "- Design a REST/gRPC service that reads from Iceberg via Spark or Trino to serve hotel insights.\n",
        "- Keep the architecture grounded in the existing dataset (no new storages).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔐 Section I — Privacy & Compliance on Generated Tables\n",
        "\n",
        "### I1. PII Masking\n",
        "- Re-write `reviews` masking emails/phones using regex UDFs; store results in `reviews_redacted`.\n",
        "\n",
        "### I2. Auditing Access\n",
        "- Capture query history against `hotels_practice` tables by parsing Spark event logs or Trino query logs (simulated).\n",
        "\n",
        "### I3. Data Retention Drill\n",
        "- Implement a delete workflow for a user (`user_id`) across bookings/reviews/images inside Iceberg, preserving an audit trail table.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
